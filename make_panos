#!/bin/env python3

import os
try:
    import subprocess
    import platform
    import sys
except KeyboardInterrupt:
    print("You pressed CTRL-c")
    sys.exit(0)

try:
    import venv
except ModuleNotFoundError:
    print("venv not found. Is python3-venv installed?")
    sys.exit(1)

from pathlib import Path

VENV_PATH = Path.home() / ".panorama_helper"
PYTHON_BIN = VENV_PATH / "bin" / "python"

if platform.system() == "Windows":
    PYTHON_BIN = VENV_PATH / "Scripts" / "python.exe"

def create_and_setup_venv():
    venv.create(VENV_PATH, with_pip=True)
    subprocess.check_call([PYTHON_BIN, "-m", "pip", "install", "--upgrade", "pip", "networkx", "rich", "opencv-python", "mypydie", "matplotlib", "sixel"])

def restart_with_venv():
    try:
        result = subprocess.run(
            [str(PYTHON_BIN)] + sys.argv,
            text=True,
            check=True,
            env=dict(**os.environ)
        )
        sys.exit(result.returncode)
    except subprocess.CalledProcessError as e:
        print(f"Exit-Code: {e.returncode}")
        sys.exit(e.returncode)
    except Exception as e:
        print(f"Unexpected error while restarting python: {e}")
        sys.exit(1)

if not os.path.exists(VENV_PATH):
    create_and_setup_venv()
    restart_with_venv()

try:
    from rich.console import Console

    console = Console()

    with console.status("[bold green]Loading modules..."):
        import argparse
        import cv2
        import json
        import numpy as np
        import subprocess
        import tempfile
        import hashlib
        import os
        import pickle
        from pathlib import Path
        from shutil import which
        from rich.text import Text
        from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
        from rich.progress import track
        from rich.panel import Panel
        from rich.progress import Progress, BarColumn, TimeElapsedColumn, TimeRemainingColumn
        from rich import print
        import networkx as nx
        from PIL import Image, ImageOps
        from matplotlib.offsetbox import OffsetImage, AnnotationBbox
        import matplotlib.pyplot as plt
        import os
        import uuid
        from mypydie import dier
        from rich.table import Table
        from concurrent.futures import ProcessPoolExecutor, as_completed, wait, FIRST_COMPLETED
        from contextlib import contextmanager
        import shlex
        from multiprocessing import cpu_count
except KeyboardInterrupt:
    print("You cancelled loading")
    sys.exit(0)
except ModuleNotFoundError:
    try:
        try:
            if not VENV_PATH.exists():
                create_and_setup_venv()
        except subprocess.CalledProcessError:
            shutil.rmtree(VENV_PATH)
            create_and_setup_venv()
            restart_with_venv()
        restart_with_venv()
    except KeyboardInterrupt:
        print("You cancelled")
        sys.exit(0)

CACHE_DIR = Path.home() / ".cache" / "feature_cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

def get_cache_key(img_path, detector_name):
    img_path = os.path.abspath(img_path)
    try:
        # Optional: √Ñnderungszeit hinzuf√ºgen, damit Cache bei Datei√§nderung invalidiert wird
        mtime = os.path.getmtime(img_path)
    except Exception:
        mtime = 0
    key_str = f"{img_path}:{mtime}:{detector_name}"
    return hashlib.md5(key_str.encode("utf-8")).hexdigest()

def serialize_keypoints(keypoints):
    return [
        (kp.pt, kp.size, kp.angle, kp.response, kp.octave, kp.class_id)
        for kp in keypoints
    ]

def deserialize_keypoints(data):
    return [
        cv2.KeyPoint(
            pt[0], pt[1], size, angle, response, octave, class_id
        )
        for pt, size, angle, response, octave, class_id in data
    ]

def load_from_cache(key):
    cache_path = CACHE_DIR / f"{key}.pkl"
    if cache_path.exists():
        try:
            with open(cache_path, "rb") as f:
                return pickle.load(f)
        except Exception:
            pass
    return None

def save_keypoints_to_cache(key, keypoints, descriptors):
    serialized_keypoints = serialize_keypoints(keypoints)
    save_to_cache(key, (serialized_keypoints, descriptors))

def load_keypoints_from_cache(key):
    cached = load_from_cache(key)
    if cached is not None:
        keypoints_data, descriptors = cached
        keypoints = deserialize_keypoints(keypoints_data)
        return (keypoints, descriptors)
    return None

def save_to_cache(key, data):
    cache_path = CACHE_DIR / f"{key}.pkl"
    with open(cache_path, "wb") as f:
        pickle.dump(data, f)

def process_image(img_path, feature_detector):
    detector = None

    if feature_detector == "SIFT":
        try:
            detector = cv2.SIFT_create()
        except AttributeError:
            from rich import print as rprint
            rprint("[bold red]‚ùå SIFT nicht verf√ºgbar.[/bold red] Versuche [green]'pip install opencv-contrib-python'[/green] oder wechsle zu ORB.")
            rprint("üîÅ Verwende stattdessen [cyan]ORB[/cyan].")
            detector = cv2.ORB_create()
            feature_detector = "ORB"
    elif feature_detector == "ORB":
        detector = cv2.ORB_create()
    else:
        raise ValueError("Ung√ºltiger Feature-Detektor. W√§hle 'SIFT' oder 'ORB'.")

    cache_key = get_cache_key(img_path, feature_detector)
    cached_result = load_keypoints_from_cache(cache_key)
    if cached_result:
        return (img_path, cached_result)

    try:
        # Bild laden mit PIL, EXIF-Rotation korrigieren, in OpenCV-Format konvertieren
        pil_img = Image.open(img_path)
        pil_img = ImageOps.exif_transpose(pil_img).convert("L")  # in Graustufen
        img = np.array(pil_img)

        if img is None:
            return (img_path, 'load_error')

        keypoints, descriptors = detector.detectAndCompute(img, None)
        if keypoints is not None and descriptors is not None:
            result = (keypoints, descriptors)
            save_keypoints_to_cache(cache_key, keypoints, descriptors)
            return (img_path, result)
        else:
            return (img_path, 'no_features')
    except Exception as e:
        return (img_path, f'error: {e}')

def get_image_timestamp(path):
    try:
        from PIL import Image
        img = Image.open(path)
        exif = img._getexif()
        if exif:
            date = exif.get(36867) or exif.get(306)
            if date:
                from datetime import datetime
                return datetime.strptime(date, "%Y:%m:%d %H:%M:%S").timestamp()
    except Exception:
        pass
    return os.path.getmtime(path)

def find_overlapping_pairs(image_paths, feature_detector="SIFT", min_matches=10, consecutive=False):
    import cv2, os, json, hashlib
    import numpy as np
    from multiprocessing import cpu_count
    from rich.progress import Progress, BarColumn, TimeElapsedColumn, TimeRemainingColumn, SpinnerColumn, TextColumn
    from rich.console import Console
    from PIL import Image
    from concurrent.futures import ThreadPoolExecutor, as_completed

    console = Console()

    # Caching
    serialized = json.dumps([image_paths, feature_detector, min_matches, consecutive], sort_keys=True)
    cache_key = hashlib.md5(serialized.encode('utf-8')).hexdigest()
    cached = load_from_cache(cache_key)
    if cached:
        return cached

    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True) if feature_detector == "SIFT" else cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    image_data = {}

    with Progress(
        TextColumn("üîç [yellow]Extrahiere Merkmale aus Bildern:[/yellow] [cyan]{task.description}"),
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        TextColumn("({task.completed}/{task.total})"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        transient=True
    ) as progress:
        task = progress.add_task("[cyan]Verarbeite Bilder... ", total=len(image_paths))

        from concurrent.futures import ThreadPoolExecutor, as_completed

        num_workers = max(1, os.cpu_count())

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = {executor.submit(process_image, path, feature_detector): path for path in image_paths}

            for future in as_completed(futures):
                img_path = futures[future]
                try:
                    _, result = future.result()
                except Exception as e:
                    result = f"error: {e}"

                progress.advance(task)

                if result == 'load_error':
                    console.print(f"[bold yellow]‚ö† Konnte Bild nicht laden:[/bold yellow] {img_path}")
                elif result == 'no_features':
                    console.print(f"[yellow]‚ö† Keine Merkmale gefunden in:[/yellow] {img_path}")
                elif isinstance(result, str) and result.startswith('error:'):
                    console.print(f"[red]‚ö† Fehler bei Bild:[/red] {img_path} ‚Äì {result[7:]}")
                else:
                    image_data[img_path] = result

    # Optional: Nur benachbarte vergleichen
    all_image_paths = list(image_data.keys())
    if consecutive:
        all_image_paths.sort(key=lambda p: (get_image_timestamp(p), os.path.basename(p)))

    overlapping_pairs = {}
    for i in range(len(all_image_paths)):
        path1 = all_image_paths[i]
        kp1, des1 = image_data[path1]
        overlapping_pairs.setdefault(path1, set())

        compare_indices = [i+1] if consecutive and i+1 < len(all_image_paths) else range(i+1, len(all_image_paths))
        for j in compare_indices:
            path2 = all_image_paths[j]
            kp2, des2 = image_data[path2]

            try:
                matches = bf.match(des1, des2)
                if len(matches) >= min_matches:
                    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
                    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
                    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
                    inlier_matches_count = np.sum(mask) if mask is not None else 0

                    if M is not None and inlier_matches_count >= min_matches:
                        console.log(f"[green]  ‚úÖ √úberlappung erkannt zwischen {os.path.basename(path1)} und {os.path.basename(path2)}[/green]")
                        overlapping_pairs[path1].add(path2)
                        overlapping_pairs.setdefault(path2, set()).add(path1)
                else:
                    console.log(f"  ‚ùå [dim]Zu wenige Matches ({len(matches)} < {min_matches})[/dim]")
            except Exception as e:
                console.log(f"  ‚ö† [red]Fehler bei Vergleich {path1} vs {path2}:[/red] {e}")
                continue

    save_to_cache(cache_key, overlapping_pairs)
    return overlapping_pairs

def find_connected_components(graph):
    """
    Findet zusammenh√§ngende Komponenten in einem Graphen (Bildergruppen).
    """

    visited = set()
    components = []

    total_nodes = len(graph)
    with Progress(
        TextColumn("[bold green]Suche zusammenh√§ngende Bildergruppen und f√ºge sie zusammen...[/bold green]"),
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TimeElapsedColumn(),
        transient=True
    ) as progress:
        for node in graph:
            if node not in visited:
                current_component = []
                stack = [node]
                visited.add(node)

                while stack:
                    vertex = stack.pop()
                    current_component.append(vertex)

                    for neighbor in graph.get(vertex, set()):
                        if neighbor not in visited:
                            visited.add(neighbor)
                            stack.append(neighbor)

                components.append(current_component)

                if len(current_component) > 1:
                    console.print("")
                    console.print(Panel.fit(
                        f"[green]üì∏ Stitching-Gruppe:[/green] {', '.join(os.path.basename(p) for p in current_component)}",
                        title=f"[bold]Gruppe mit {len(current_component)} Bild(en)[/bold]",
                        subtitle="Stitching wird gestartet...",
                        border_style="green"
                    ))

                    stitch_image_group(current_component, progress)

    console.print(f"[bold cyan]‚úÖ Insgesamt {len(components)} Komponente(n) gefunden.[/bold cyan]")
    return components

def group_images_by_overlap_and_merge_when_found(directory, feature_detector="SIFT", min_matches=10, consecutive=False):
    """
    Hauptfunktion zum Gruppieren von Bildern basierend auf √úberlappungen.
    """
    image_paths = []
    console.print(Panel(f"[bold]Durchsuche Verzeichnis:[/bold] [cyan]{directory}[/cyan]"))

    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')) and not file.lower().startswith("pano_"):
                image_paths.append(os.path.join(root, file))

    if not image_paths:
        console.print(f"[bold red]‚ùå Keine Bilder im Verzeichnis '{directory}' gefunden.[/bold red]")
        return []

    console.print(Panel.fit(
        f"[bold]üîç Starte √úberlappungssuche[/bold]\n[green]Detektor:[/green] {feature_detector}\n[green]Minimale Matches:[/green] {min_matches}",
        title="√úberlappungssuche"
    ))

    # Schritt 1: Finde alle Paare, die sich √ºberlappen
    overlapping_graph = find_overlapping_pairs(image_paths, feature_detector, min_matches, consecutive)

    # Schritt 2: Finde zusammenh√§ngende Komponenten im √úberlappungsgraphen
    grouped_images = find_connected_components(overlapping_graph)

    # Filter: Nur Gruppen mit mehr als einem Bild sind interessant f√ºr Panoramen
    final_groups = [group for group in grouped_images if len(group) > 1]

    if not final_groups:
        console.print("[yellow] Keine Gruppen √ºberlappender Bilder (mit mehr als einem Bild) gefunden.[/yellow]")
    else:
        table = Table(title="Panorama-Gruppen", show_lines=True)
        table.add_column("Gruppe", justify="right")
        table.add_column("Bildanzahl", justify="center")
        table.add_column("Bilder", justify="left")

        for i, group in enumerate(final_groups, 1):
            table.add_row(
                f"[cyan]#{i}[/cyan]",
                str(len(group)),
                "\n".join(os.path.basename(p) for p in group)
            )
        console.print(f"[bold green]‚ú® {len(final_groups)} Panorama-Gruppen gefunden.[/bold green]")
        console.print(table)

    return final_groups

def run(cmd, check=True):
    try:
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode != 0:
            print(f"‚ùå Fehler bei: {' '.join(cmd)}")
            print(result.stderr.strip())
            if check:
                raise RuntimeError(f"Command failed: {' '.join(cmd)}")
            return False
        return True
    except ChildProcessError as e:
        print("Process failed or was cancelled")
    return None

@contextmanager
def progress_task(progress, description, **kwargs):
    task_id = progress.add_task(description, **kwargs)
    try:
        progress.start_task(task_id)
        yield task_id
        # Task sicher aus Liste finden
        task = next(t for t in progress.tasks if t.id == task_id)
        if task.total is not None:
            progress.update(task_id, completed=task.total)
    except Exception:
        progress.update(task_id, description=f"[red]‚ùå {description}")
        raise
    finally:
        progress.remove_task(task_id)

def has_control_points(pto_path):
    try:
        with open(pto_path) as f:
            return any(line.startswith("c ") for line in f)
    except Exception:
        return False

def stitch_image_group(images, progress, output_dir="."):
    if len(images) < 2:
        console.print("‚ùå [red]Zu wenige Bilder f√ºr ein Panorama.[/red]")
        return None

    joined = "\n".join(images)
    hash_str = hashlib.sha1(joined.encode()).hexdigest()
    jpg_name = f"pano_{hash_str}.jpg"
    jpg_path = os.path.join(output_dir, jpg_name)

    if os.path.exists(jpg_path):
        console.print(f"[yellow]Die Datei {jpg_path} existiert bereits.[/yellow]")
        return

    tmpdir = tempfile.mkdtemp()
    pto = os.path.join(tmpdir, f"pano_{hash_str}.pto")
    tif_prefix = os.path.join(tmpdir, f"pano_{hash_str}")
    images = sorted(map(str, images))

    console.print("")
    console.rule(f"üßµ Gruppe mit {len(images)} Bildern ‚Üí Hash: [cyan]{hash_str}[/cyan]")

    if args.max_group_nr < len(images):
        console.print(f"Image group too large: {len(images)}")
        return

    steps = [
        ("pto_gen", ["pto_gen", *images, "-o", pto]),
        ("cpfind", ["cpfind", "--multirow", "-o", pto, pto]),
    ]

    if has_control_points(pto):
        steps += [
            ("autooptimiser", ["autooptimiser", "-a", "-l", "-s", "-m", "-o", pto, pto]),
            ("celeste_standalone", ["celeste_standalone", "-i", pto, "-o", pto]),
        ]
    else:
        print("‚ö†Ô∏è  Warning: No control points found in PTO file, skipping autooptimiser + celeste.")

    steps += [
        ("pano_modify", ["pano_modify", "-o", pto, "--canvas=AUTO", "--crop=AUTO", pto]),
        ("hugin_executor", ["hugin_executor", "--stitching", f"--prefix={tif_prefix}", pto]),
    ]

    for name, cmd in steps:
        with progress_task(progress, f"[green]‚û° {name} ...", spinner="dots"):
            try:
                run(cmd, check=True)
            except ChildProcessError:
                progress.update(new_task, description=f"[red]‚ùå {name} fehlgeschlagen")
                return None

    tif_file = next(Path(tmpdir).glob(f"pano_{hash_str}*.tif"), None)
    if not tif_file or not tif_file.exists():
        console.print(f"[red]‚ùå Kein .tif gefunden unter [bold]{tmpdir}[/bold][/red]")
        return None

    try:
        with progress_task(progress, "‚û° [cyan]Konvertiere zu JPG...[/cyan]", spinner="dots"):
            run(["convert", str(tif_file), "-quality", "92", jpg_path], check=True)
            tif_file.unlink()
    except ChildProcessError:
        console.print("[red]‚ùå convert fehlgeschlagen[/red]")
        return None

    return jpg_path

def group_images_in_all_subdirs(base_dir, detector, min_matches, consecutive=False):
    original_dir = os.getcwd()  # Merke Ursprungs-Verzeichnis

    for root, dirs, files in os.walk(base_dir):
        group_images_by_overlap_and_merge_when_found(os.path.abspath(root), detector, min_matches, consecutive)

if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser(description="Gruppiert Bilder basierend auf tats√§chlichen Inhalts√ºberlappungen f√ºr Panorama-Stitching.")
        parser.add_argument("directory", nargs="?", default=".", 
                            help="Der Pfad zum Verzeichnis mit den Bildern. Standard ist das aktuelle Verzeichnis.")
        parser.add_argument("--detector", type=str, default="ORB", choices=["SIFT", "ORB"],
                            help="Der zu verwendende Feature-Detektor (SIFT oder ORB). SIFT ist genauer, aber ORB ist schneller.")
        parser.add_argument("--max-group-nr", type=int, default=6,
                            help="Die minimale Anzahl von √ºbereinstimmenden Merkmalen, um eine √úberlappung anzunehmen (Standard: 20).")
        parser.add_argument("--min-matches", type=int, default=8,
                            help="Die minimale Anzahl von √ºbereinstimmenden Merkmalen, um eine √úberlappung anzunehmen (Standard: 20).")
        parser.add_argument("--consecutive", action="store_true", help="Nur aufeinanderfolgende Bilder vergleichen (nach Zeit und Dateiname sortiert)")

        args = parser.parse_args()

        group_images_in_all_subdirs(args.directory, args.detector, args.min_matches, args.consecutive)
    except KeyboardInterrupt:
        console.print("[yellow]You cancelled[/yellow]")
